{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE - UY 4563: Intro to Machine Learning ~> Seg_net\n",
    "\n",
    "#### Kyle Ong\n",
    "#### Rohan Chakraborty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Flavors of image segmentation networks such as RCNN require complicated architectures including resnet backbones and feature pyramids. We propse a lightweight architecture for image segmentation consisting of a convolution fed into hebian weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seg_net model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://saytosid.github.io/images/segnet/Complete%20architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch ~> python deep learning library  \n",
    "\n",
    "torch.nn ~>  the base class for all neural network modules\n",
    "\n",
    "numpy ~> scientific computing with python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import math\n",
    "import torch.nn.functional as Funct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### down_block       -        a single encoder block of the Segnet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`down_block` class implements the convulutional, batch normalisation and ReLU encoder layers. The class initializes either 2 or 3 convolutional layers. `forward` function implements the data-flow through a single encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class down_block(nmModule):\n",
    "''''\n",
    "    decoder of seg_net consists of convolution, batch normalisation,maxpool,  ReLu \n",
    "    convolution performs a (3,3) kernel with stride of 1\n",
    "    maxpool perfroms a (2,2) kernel with a stride of  2\n",
    "''''\n",
    "\n",
    "    def __init__(self):\n",
    "    '''\n",
    "        intialize convolutional layers\n",
    "    '''\n",
    "    \n",
    "    def forward(self, x, indices, size):\n",
    "    '''\n",
    "        perform a single pass through an encoder\n",
    "       \n",
    "        returns a tensor with decreased dimensions, the maxpool indices, and size\n",
    "    '''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class down_block(nn.Module):\n",
    "    #using the input channels I specify the channels for repeated use of this block\n",
    "    def __init__(self, channels, num_of_convs = 2):\n",
    "        super(down_block,self).__init__()\n",
    "\n",
    "        self.num_of_convs = num_of_convs\n",
    "\n",
    "        # Declare operations with learning features\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel_size=(3,3),stride=1,padding=0,dilation=0,bias=True)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(channels[1])\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[1], kernel_size=(3,3),stride=1,padding=0,dilation=0,bias=True)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(channels[1])\n",
    "        if(num_of_convs == 3):\n",
    "            self.conv3 = nn.Conv2d(channels[1], channels[1], kernel_size=(3,3),stride=1,padding=0,dilation=0,bias=True)\n",
    "            self.batchnorm3 = nn.BatchNorm2d(channels[1])\n",
    "\n",
    "        # Declare operations without learning features\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=2, return_indices = True)\n",
    "        \n",
    "        # Initialize Kernel weights for the encoder section with vgg weights\n",
    "        # this will be done on another python file after an instance of the model network is created\n",
    "                \n",
    "    #forward function through the block\n",
    "    def forward(self, x):\n",
    "        input_size = x.size()\n",
    "        \n",
    "        fwd_map = self.conv1(x)\n",
    "        fwd_map = self.batchnorm1(fwd_map)\n",
    "        self.relu(fwd_map)\n",
    "\n",
    "        fwd_map = self.conv2(fwd_map)\n",
    "        fwd_map = self.batchnorm2(fwd_map)\n",
    "        self.relu(fwd_map)\n",
    "\n",
    "        if(self.num_of_convs == 3):\n",
    "            fwd_map = self.conv3(fwd_map)\n",
    "            fwd_map = self.batchnorm3(fwd_map)\n",
    "            self.relu(fwd_map)\n",
    "\n",
    "        #Saving the tensor and for unpooling tensor size & indeces to map it to the layers deeper in the model\n",
    "        output_size = fwd_map.size()\n",
    "        fwd_map, indices = self.maxpool(fwd_map)\n",
    "        \n",
    "        size = {\"input_size\": input_size, \"b4max\": output_size}\n",
    "        return (fwd_map, indices, size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  up_block       -        a single decoder block of the Segnet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`up_block` class implements convolution, batch normalisation, ReLu, and softmax. The class initializes either 2 or 3 convultional layers.  `forward` implements the data-flow through a single decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class up_block(nmModule):\n",
    "''''\n",
    "    decoder of seg_net consists of convolution, batch normalisation, ReLu and softmax\n",
    "    convolution performs a (3,3) kernel with stride of 1\n",
    "''''\n",
    "\n",
    "    def __init__(self):\n",
    "    '''\n",
    "        intialize convolutional layers\n",
    "    '''\n",
    "    \n",
    "    def forward(self, x, indices, size):\n",
    "    '''\n",
    "        perform a single pass through  a decoder\n",
    "             \n",
    "        returns a tensor with increased dimensions\n",
    "        \n",
    "    '''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class up_block(nn.Module):\n",
    "\n",
    "    def __init__(self,channels,num_of_convs = 2):\n",
    "        super(up_block,self).__init__()\n",
    "        \n",
    "        self.num_of_convs = num_of_convs\n",
    "        \n",
    "        self.unpooled = nn.MaxUnpool2d(kernel_size=(2,2) , stride=2)\n",
    "        self.upsample = nn.upsample(mode=\"bilinear\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel_size=(3,3), stride=1, padding=0, dilation=0, bias=True)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(channels[1])\n",
    "        \n",
    "        if(num_of_convs== 2):\n",
    "            self.conv2 = nn.Conv2d(channels[1], channels[1], kernel_size=(3,3), stride=1, padding=0, dilation=0, bias=True)\n",
    "        elif(num_of_convs == 3):\n",
    "            self.conv2 = nn.Conv2d(channels[1], channels[1], kernel_size=(3,3), stride=1, padding=0, dilation=0, bias=True)\n",
    "            self.batchnorm2 = nn.BatchNorm2d(channels[1])\n",
    "            self.conv3 = nn.Conv2d(channels[1], channels[1], kernel_size=(3,3), stride=1, padding=0, dilation=0, bias=True)\n",
    "        \n",
    "        self.batchnorm_for_last_conv = nn.BatchNorm2d(channels[1])\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "        # Initialize Kernel weights for the decoder section with normally distributed weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "                \n",
    "    #forward function through the block\n",
    "    def forward(self, x, indices, size):\n",
    "\n",
    "        #print(\"Before upsampling: \", x.size())\n",
    "        fwd_map = self.unpooled(x, indices, output_size=size)\n",
    "        fwd_map = self.upsample(fwd_map)\n",
    "        \n",
    "        fwd_map = self.conv1(fwd_map)\n",
    "        fwd_map = self.batchnorm1(fwd_map)\n",
    "        self.relu(fwd_map)\n",
    "        \n",
    "        if(self.num_of_convs == 2):\n",
    "            fwd_map = self.conv2(fwd_map)\n",
    "            fwd_map = self.batchnorm_for_last_conv(fwd_map)\n",
    "            self.relu(fwd_map)\n",
    "\n",
    "        elif(self.num_of_convs == 3):\n",
    "            fwd_map = self.conv2(fwd_map)\n",
    "            fwd_map = self.batchnorm2(fwd_map)\n",
    "            self.relu(fwd_map)\n",
    "\n",
    "            fwd_map = self.conv3(fwd_map)\n",
    "            fwd_map = self.batchnorm_for_last_conv(fwd_map)\n",
    "            self.relu(fwd_map)\n",
    "\n",
    "        #print(\"down block after convs: \", fwd_map.size())\n",
    "        \n",
    "        return fwd_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`network` class implements the convolutional network with up_blocks and down_blocks. The class intializes ten layers. The first ten layers are `down_blocks` and the rest are `up_blocks`. `Forward` function implements a single pass through seg_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class network(nmModule):\n",
    "\n",
    "    def __init__(self):\n",
    "    '''\n",
    "        intialize the layers of the network\n",
    "    '''\n",
    "    \n",
    "    def forward(self,x):\n",
    "    '''\n",
    "        perform a single pass through the network\n",
    "        \n",
    "        returns the output of softmax with k-channels\n",
    "        where each channel represents a probability distribution of the corresponding label\n",
    "        \n",
    "    '''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(network,self).__init__()\n",
    "        self.layer1 = down_block((3,64), 2)              \n",
    "        self.layer2 = down_block((64,128), 2)\n",
    "        self.layer3 = down_block((128,256), 3)\n",
    "        self.layer4 = down_block((256,512), 3)\n",
    "        self.layer5 = down_block((512,1024), 3)\n",
    "        \n",
    "        #self.layer6 = up_block((inp,curr,next), 3)\n",
    "        self.layer6 = up_block((512,1024), 3)\n",
    "        self.layer7 = up_block((512,256), 3)\n",
    "        self.layer8 = up_block((256,128), 3)\n",
    "        self.layer9 = up_block((128,64), 2)\n",
    "        self.layer10 = up_block((64,1), 2)\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(64, 35, kernel_size=(1,1), stride = 1, padding=0, dilation=0, bias=False)\n",
    "        self.softmax = nn.Softmax(dim   = 2 )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        #print(\"\\nLayer1...\")\n",
    "        out1, indices1, size1= self.layer1(x)\n",
    "        #print(\"in forward \", Funct.softmax(out1).size())\n",
    "        #print(\"\\nLayer2...\")\n",
    "        out2, indices2, size2 = self.layer2(out1)\n",
    "        #print(\"\\nLayer3...\")\n",
    "        out3, indices3, size3= self.layer3(out2)\n",
    "        #print(\"\\nLayer4...\")\n",
    "        out4, indices4,size4 = self.layer4(out3)\n",
    "        #print(\"\\nLayer5...\")\n",
    "        out5, indices5, size5 = self.layer5(out4)\n",
    "\n",
    "        #print(\"\\nLayer6...\")\n",
    "        out6 = self.layer6(out5, indices5, size5['b4max'])\n",
    "        #print(\"\\nLayer7...\")\n",
    "        out7 = self.layer7(out6, indices4, size4['b4max'])\n",
    "        #print(\"\\nLayer8...\")\n",
    "        out8 = self.layer8(out7, indices3, size3['b4max'])\n",
    "        #print(\"\\nLayer9...\")\n",
    "        out9 = self.layer9(out8, indices2, size2['b4max'])\n",
    "        #print(\"\\nLayer10...\")\n",
    "        out10 = self.layer10(out9, indices1, size1['b4max'])\n",
    "        \n",
    "        print(\"\\nconv1x1\")\n",
    "        out_conv1x1 = self.conv1x1(out10)\n",
    "        \n",
    "        #print(out10)\n",
    "        #print(\"size of out10:\", out10.size())\n",
    "        #print(\"\\nSoftmax Layer...\")\n",
    "        #res = Funct.softmax(out10)\n",
    "        \n",
    "        \n",
    "        res = self.softmax(out10)\n",
    "\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
