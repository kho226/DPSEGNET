{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from Segnet import network\n",
    "from data_loader import data_loader_seg\n",
    "# from LoadWeights import preload_encoder_weights\n",
    "\n",
    "import torch \n",
    "import numpy as np \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets,models,transforms\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# A list of all labels\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Create dictionaries for a fast lookup\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please refer to the main method below for example usages!\n",
    "\n",
    "# name to label object\n",
    "# name2label      = { label.name    : label for label in labels           }\n",
    "\n",
    "# id to label object\n",
    "# id2label        = { label.id      : label for label in labels           }\n",
    "\n",
    "# trainId to label object\n",
    "# trainId2label   = { label.trainId : label for label in reversed(labels) }\n",
    "\n",
    "# color to label object \n",
    "color2label = { label.color : label for label in labels }\n",
    "\n",
    "# category to list of label objects\n",
    "# category2labels = {}\n",
    "# for label in labels:\n",
    "#     category = label.category\n",
    "#     if category in category2labels:\n",
    "#         category2labels[category].append(label)\n",
    "#     else:\n",
    "#         category2labels[category] = [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = network(35)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    model_ft = model_ft.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload_encoder_weights(model_ft)\n",
    "\n",
    "#APPLY TRANSFORM IF NEEDED\n",
    "trans = transforms.Compose([ \n",
    "    transforms.CenterCrop((1200, 350)), \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_train = data_loader_seg('./Dataset/data_semantics/training/',trans=None, c2id=color2label)\n",
    "dsets_enqueuer_training = torch.utils.data.DataLoader(dsets_train, batch_size=1, num_workers=10, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, data in enumerate(dsets_enqueuer_training, 1):\n",
    "       #image,image_seg = data['image'], data['image_seg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_ft.parameters(),lr = 0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "loss_data = 0.0\n",
    "loss_data_testing = 0.0\n",
    "\n",
    "loss_per_epoch_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "......Training......\n",
      "<class 'torch.FloatTensor'>\n",
      "<class 'torch.FloatTensor'>\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "\n",
      "Layer1...\n",
      "torch.Size([1, 64, 173, 613])\n",
      "\n",
      "Layer2...\n",
      "torch.Size([1, 128, 84, 304])\n",
      "\n",
      "Layer3...\n",
      "torch.Size([1, 256, 39, 149])\n",
      "\n",
      "Layer4...\n",
      "torch.Size([1, 512, 16, 71])\n",
      "\n",
      "Layer7...\n",
      "torch.Size([1, 256, 39, 149])\n",
      "\n",
      "Layer8...\n",
      "torch.Size([1, 128, 84, 304])\n",
      "\n",
      "Layer9...\n",
      "torch.Size([1, 64, 173, 613])\n",
      "\n",
      "Layer10...\n",
      "torch.Size([1, 64, 350, 1230])\n",
      "\n",
      "conv1x1\n",
      "size of out_conv1x1: torch.Size([1, 35, 350, 1230])\n",
      "\n",
      "Softmax Layer...\n",
      "\n",
      "\n",
      "DONE......\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\n......Training......\")\n",
    "loss_lst_train = []\n",
    "loss_lst_test = []\n",
    "\n",
    "\n",
    "\n",
    "for Epoch in range(100):\n",
    "    \n",
    "    for idx, data in enumerate(dsets_enqueuer_training, 1):\n",
    "        #implements a batch process rather than individual processsing images\n",
    "        # resulting in (1, 375 ,1242, 3)\n",
    "        # we need (1, 375, 1242)\n",
    "        \n",
    "        image,image_seg = data['image'], data['image_seg']\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        print(type(image))\n",
    "        print(type(image_seg))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            image, image_seg = Variable(image.cuda(), requires_grad = False), Variable(image_seg.cuda(), requires_grad = False)\n",
    "        else:\n",
    "            image, image_seg = Variable(image, requires_grad = False), Variable(image_seg, requires_grad = False)\n",
    "\n",
    "       \n",
    "        print(type(image))\n",
    "        model_ft.train(True)\n",
    "        output = model_ft(image)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #loss = criterion(output,image_seg)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        #loss_data += loss.data\n",
    "        \n",
    "        break\n",
    "    break\n",
    "print(\"\\n\\nDONE......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "print (type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0  , 0  ,.,.) = \n",
      "  35  35  17  ...   17  23  35\n",
      "   6  28  35  ...   35  35  35\n",
      "  35   6  24  ...   35  23  35\n",
      "     ...       â‹±       ...    \n",
      "  24  17  17  ...    6  17  30\n",
      "  35  24   6  ...   30  30  30\n",
      "  29  24  28  ...   35  35  35\n",
      "[torch.LongTensor of size 1x1x350x1230]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
